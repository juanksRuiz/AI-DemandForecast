{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\juanc\\AppData\\Local\\Temp\\ipykernel_25076\\1703249404.py:1: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  demanda_df = pd.read_csv(\".\\Datos\\demanda.csv\")\n"
     ]
    }
   ],
   "source": [
    "demanda_df = pd.read_csv(\".\\Datos\\demanda.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juanc\\AppData\\Local\\Temp\\ipykernel_25076\\1081254819.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  demanda_prod_df['date'] = pd.to_datetime(demanda_prod_df['date'], format='%Y-%m-%d')\n"
     ]
    }
   ],
   "source": [
    "demanda_prod_df = demanda_df[demanda_df['id_producto']==1]\n",
    "demanda_prod_df['date'] = pd.to_datetime(demanda_prod_df['date'], format='%Y-%m-%d')\n",
    "\n",
    "# Primero, asegurar que todas las fechas estan\n",
    "# Los datos de demanda iniciales no tienen enero 2020\n",
    "fecha_inicial = pd.to_datetime('2020-01-01')\n",
    "fechas_df = pd.DataFrame({'date': pd.date_range(start=fecha_inicial, end=demanda_prod_df['date'].max())})\n",
    "complete_demanda_prod_df = fechas_df.merge(demanda_prod_df\n",
    "                                           ,how='left'\n",
    "                                           ,left_on='date'\n",
    "                                           ,right_on='date'\n",
    "                                           )\n",
    "\n",
    "\n",
    "complete_demanda_prod_df['anio'] = complete_demanda_prod_df['date'].dt.year\n",
    "complete_demanda_prod_df['mes'] = complete_demanda_prod_df['date'].dt.month\n",
    "complete_demanda_prod_df['dia'] = complete_demanda_prod_df['date'].dt.day\n",
    "complete_demanda_prod_df['dia_de_semana'] = complete_demanda_prod_df['date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juanc\\AppData\\Local\\Temp\\ipykernel_25076\\3688260578.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  complete_demanda_prod_df['demanda'].ffill(inplace=True)\n",
      "C:\\Users\\juanc\\AppData\\Local\\Temp\\ipykernel_25076\\3688260578.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  complete_demanda_prod_df['demanda'].bfill(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Preprocesamiento de la serie de tiempo\n",
    "complete_demanda_prod_df['demanda'].ffill(inplace=True)\n",
    "complete_demanda_prod_df['demanda'].bfill(inplace=True)\n",
    "\n",
    "\n",
    "# Eliminacion de 29 de febrero.\n",
    "complete_demanda_prod_df = complete_demanda_prod_df.sort_values(by='date', ascending=True)\n",
    "complete_demanda_prod_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Crear una columna indicadora para el 29 de febrero\n",
    "complete_demanda_prod_df['is_feb_29'] = complete_demanda_prod_df['date'].apply(lambda x: x.month == 2 and x.day == 29)\n",
    "complete_demanda_prod_df = complete_demanda_prod_df[~complete_demanda_prod_df['is_feb_29']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creacion de variables exogenas\n",
    "# Encoding ciclico para variables ciclicas\n",
    "complete_demanda_prod_df['mes_sin'] = np.sin(2 * np.pi *complete_demanda_prod_df['mes'] / 12)\n",
    "complete_demanda_prod_df['mes_cos'] = np.cos(2 * np.pi *complete_demanda_prod_df['mes'] / 12)\n",
    "\n",
    "complete_demanda_prod_df['dia_sin'] = np.sin(2 * np.pi *complete_demanda_prod_df['dia'] / 31)\n",
    "complete_demanda_prod_df['dia_cos'] = np.cos(2 * np.pi *complete_demanda_prod_df['dia'] / 31)\n",
    "\n",
    "complete_demanda_prod_df['dia_de_semana_sin'] = np.sin(2 * np.pi *complete_demanda_prod_df['dia_de_semana'] / 7)\n",
    "complete_demanda_prod_df['dia_de_semana_cos'] = np.cos(2 * np.pi *complete_demanda_prod_df['dia_de_semana'] / 7)\n",
    "\n",
    "# Datos booleanos transformados a enteros 1 o 0\n",
    "# Flag de datos despues de aparicion de tienda de la competencia\n",
    "fecha_cambio = pd.Timestamp(\"2021-07-02\")\n",
    "complete_demanda_prod_df[\"flg_date_despues_tienda_competencia\"] = (complete_demanda_prod_df[\"date\"] >= fecha_cambio).astype(int)\n",
    "\n",
    "complete_demanda_prod_df['es_fin_de_semana'] = complete_demanda_prod_df['date'].dt.weekday.isin([5,6]).astype(int)\n",
    "complete_demanda_prod_df['es_fin_de_semana'] = complete_demanda_prod_df['es_fin_de_semana'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separacion de datos antes y despues\n",
    "antes_comp_df = complete_demanda_prod_df[complete_demanda_prod_df['date'] < '2021-07-02']\n",
    "antes_comp_df.reset_index(inplace=True, drop=True)\n",
    "despues_comp_df = complete_demanda_prod_df[complete_demanda_prod_df['date'] >= '2021-07-02']\n",
    "despues_comp_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "exog_cols = ['es_fin_de_semana'\n",
    "             , 'mes_sin', 'mes_cos', 'dia_sin', 'dia_cos', 'dia_de_semana_sin','dia_de_semana_cos']\n",
    "X_exog = complete_demanda_prod_df.loc[:, exog_cols]\n",
    "X_exog_antes = antes_comp_df.loc[:, exog_cols]\n",
    "X_exog_despues = despues_comp_df.loc[:, exog_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluacion de la descomposicion de la serie con STL\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluemos si con los datos despues de la competencia, se necesita diferenciacion y si se necesita aplicar logaritmo para reducir la varianza\n",
    "\n",
    "# Evaluacion de necesidad de diferenciacion:\n",
    "# Diferenciacion de datos antes de aparicion de tienda de la competencia\n",
    "\n",
    "# Aplicar la primera diferenciación\n",
    "despues_comp_df['demanda_diff_1'] = despues_comp_df['demanda'].diff()\n",
    "\n",
    "# Eliminar valores NaN generados por la diferenciación\n",
    "despues_comp_df.dropna(inplace=True)\n",
    "despues_comp_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Graficar la serie diferenciada\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(despues_comp_df['demanda_diff_1'], label='Serie diferenciada (1ra orden)')\n",
    "plt.legend()\n",
    "plt.title('Serie Diferenciada - Primera Diferenciación')\n",
    "plt.show()\n",
    "\n",
    "# Prueba ADF en la serie diferenciada\n",
    "adf_result = adfuller(despues_comp_df['demanda_diff_1'])\n",
    "print(f'ADF Statistic: {adf_result[0]}')\n",
    "print(f'p-value: {adf_result[1]}')\n",
    "print(f'Critical Values:')\n",
    "for key, value in adf_result[4].items():\n",
    "    print(f'   {key}: {value}')\n",
    "\n",
    "# Verificar si aún no es estacionaria (p-valor > 0.05), aplicar segunda diferenciación\n",
    "if adf_result[1] > 0.05:\n",
    "    print(\"\\nLa serie sigue sin ser estacionaria. Aplicando segunda diferenciación...\\n\")\n",
    "\n",
    "    # Segunda diferenciación\n",
    "    despues_comp_df['demanda_diff_2'] = despues_comp_df['demanda_diff_1'].diff()\n",
    "    despues_comp_df.dropna(inplace=True)\n",
    "    despues_comp_df.reset_index(inplace=True, drop=True)\n",
    "    # Graficar la serie diferenciada (2da orden)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(despues_comp_df['demanda_diff_2'], label='Serie diferenciada (2da orden)')\n",
    "    plt.legend()\n",
    "    plt.title('Serie Diferenciada - Segunda Diferenciación')\n",
    "    plt.show()\n",
    "\n",
    "    # Prueba ADF en la serie diferenciada (2da orden)\n",
    "    adf_result_2 = adfuller(df['demanda_diff_2'])\n",
    "    print(f'ADF Statistic (2da orden): {adf_result_2[0]}')\n",
    "    print(f'p-value: {adf_result_2[1]}')\n",
    "    print(f'Critical Values:')\n",
    "    for key, value in adf_result_2[4].items():\n",
    "        print(f'   {key}: {value}')\n",
    "\n",
    "# Verifica si ahora la serie es estacionaria\n",
    "if adf_result[1] < 0.05:\n",
    "    print(\"\\n✅ La serie se volvió estacionaria con una sola diferenciación.\")\n",
    "elif 'adf_result_2' in locals() and adf_result_2[1] < 0.05:\n",
    "    print(\"\\n✅ La serie se volvió estacionaria con dos diferenciaciones.\")\n",
    "else:\n",
    "    print(\"\\n⚠️ La serie aún no es estacionaria. Considera otras transformaciones (log, sqrt).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sabemos que los datos despues del 2 de julio no requieren transformaciones iniciales con la prueba de Levene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validacion de si es necesario algun componente estacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stl = STL(despues_comp_df['demanda'], period=7)\n",
    "decomposicion = stl.fit()\n",
    "\n",
    "# Graficar los componentes de la descomposición\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Observación original\n",
    "plt.subplot(411)\n",
    "plt.plot(despues_comp_df['date'], decomposicion.observed)\n",
    "# Formatear los ticks para que muestren solo el mes y el anio\n",
    "plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m'))\n",
    "plt.title('Observación')\n",
    "\n",
    "# Tendencia\n",
    "plt.subplot(412)\n",
    "plt.plot(despues_comp_df['date'], decomposicion.trend)\n",
    "# Formatear los ticks para que muestren solo el mes y el anio\n",
    "plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m'))\n",
    "plt.title('Tendencia')\n",
    "\n",
    "# Estacionalidad\n",
    "plt.subplot(413)\n",
    "plt.plot(despues_comp_df['date'], decomposicion.seasonal)\n",
    "# Formatear los ticks para que muestren solo el mes y el anio\n",
    "plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m'))\n",
    "plt.title('Estacionalidad')\n",
    "\n",
    "# Residual\n",
    "plt.subplot(414)\n",
    "plt.plot(despues_comp_df['date'], decomposicion.resid)\n",
    "# Formatear los ticks para que muestren solo el mes y el anio\n",
    "plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m'))\n",
    "plt.title('Residual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomposicion con estacionalidad de 365 dias\n",
    "stl = STL(despues_comp_df['demanda'], period=365)\n",
    "decomposicion = stl.fit()\n",
    "\n",
    "# Graficar los componentes de la descomposición\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Observación original\n",
    "plt.subplot(411)\n",
    "plt.plot(despues_comp_df['date'], decomposicion.observed)\n",
    "# Formatear los ticks para que muestren solo el mes y el anio\n",
    "plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m'))\n",
    "plt.title('Observación')\n",
    "\n",
    "# Tendencia\n",
    "plt.subplot(412)\n",
    "plt.plot(despues_comp_df['date'], decomposicion.trend)\n",
    "# Formatear los ticks para que muestren solo el mes y el anio\n",
    "plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m'))\n",
    "plt.title('Tendencia')\n",
    "\n",
    "# Estacionalidad\n",
    "plt.subplot(413)\n",
    "plt.plot(despues_comp_df['date'], decomposicion.seasonal)\n",
    "# Formatear los ticks para que muestren solo el mes y el anio\n",
    "plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m'))\n",
    "plt.title('Estacionalidad')\n",
    "\n",
    "# Residual\n",
    "plt.subplot(414)\n",
    "plt.plot(despues_comp_df['date'], decomposicion.resid)\n",
    "# Formatear los ticks para que muestren solo el mes y el anio\n",
    "plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m'))\n",
    "plt.title('Residual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validacion cruzada comparando modelo ARIMA vs SARIMA (`s=7`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcion 1, lo hacemos desde 0 con grid_search\n",
    "import itertools\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA solo usa (p,d,q)\n",
    "param_grid_arima = [(2, 1, q) for q in [7, 14, 21, 28]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de valores permitidos para q\n",
    "q = 6 # usar este para red de parametros SARIMA\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "S = 7  # Periodo estacional\n",
    "\n",
    "\n",
    "# Diccionario con valores permitidos para Q según q\n",
    "# q_Q_permitidos = {\n",
    "#    7:  [2]#, 3, 4],   # Q ≠ 1\n",
    "#    ,8:  [1]#, 2, 3, 4], # Todos permitidos\n",
    "#    ,14: [1]#, 3, 4],   # Q ≠ 2\n",
    "#    ,21: [1]#, 2, 4],   # Q ≠ 3\n",
    "#    ,25: [1]#, 2, 3, 4]  # Todos permitidos\n",
    "#}\n",
    "\n",
    "# Generar combinaciones permitidas\n",
    "param_grid_sarima = [\n",
    "    (2, 1, q, P, 1, Q, S)\n",
    "    for P in range(1,5)\n",
    "    for Q in range(1,5)\n",
    "]\n",
    "\n",
    "# Validación cruzada\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "results_arima = []\n",
    "results_sarima = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparación ARIMA\n",
    "for p, d, q in param_grid_arima:\n",
    "    try:\n",
    "        print(f\"Ejecutando ARIMA({p},{d},{q})\")\n",
    "        errors = []\n",
    "        for train_idx, test_idx in tscv.split(despues_comp_df['demanda']):\n",
    "            train, test = despues_comp_df['demanda'][train_idx], despues_comp_df['demanda'][test_idx]\n",
    "            X_exog_train = X_exog_despues.iloc[train_idx, :]\n",
    "            X_exog_test = X_exog_despues.iloc[test_idx, :]\n",
    "\n",
    "            # Modelo ARIMA\n",
    "            modelo = sm.tsa.ARIMA(train, order=(p, d, q),\n",
    "                                  enforce_stationarity=True,\n",
    "                                  enforce_invertibility=True,\n",
    "                                  exog=X_exog_train)\n",
    "            modelo_fit = modelo.fit()\n",
    "            predictions = modelo_fit.forecast(steps=len(test), exog=X_exog_test)\n",
    "\n",
    "            # RMSE\n",
    "            split_rmse = np.sqrt(np.mean((predictions - test) ** 2))\n",
    "            errors.append(split_rmse)\n",
    "\n",
    "        avg_rmse = np.mean(errors)\n",
    "        results_arima.append(((p, d, q), avg_rmse))\n",
    "        print(f\"Finalizado ARIMA({p},{d},{q}) con RMSE: {avg_rmse}\")\n",
    "        print(\"----------------------------------\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error en ARIMA({p},{d},{q}): {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sarima = []\n",
    "\n",
    "# Comparación SARIMA\n",
    "for p, d, q, P, D, Q, s in param_grid_sarima:\n",
    "    try:\n",
    "        print(f\"Ejecutando SARIMA({p},{d},{q})({P},{D},{Q},{s})\")\n",
    "        errors = []\n",
    "\n",
    "        for train_idx, test_idx in tscv.split(despues_comp_df['demanda']):\n",
    "            train, test = despues_comp_df['demanda'][train_idx], despues_comp_df['demanda'][test_idx]\n",
    "            X_exog_train = X_exog_despues.iloc[train_idx, :]\n",
    "            X_exog_test = X_exog_despues.iloc[test_idx, :]\n",
    "            #print('X_exog_train.shape', X_exog_train.shape)\n",
    "            #print('X_exog_test.shape', X_exog_test.shape)\n",
    "\n",
    "\n",
    "            if len(train) == 0 or len(test) == 0:\n",
    "                raise ValueError(\"Conjunto de entrenamiento o prueba vacío\")\n",
    "\n",
    "            if X_exog_train.isnull().values.any() or X_exog_test.isnull().values.any():\n",
    "                raise ValueError(\"Existen valores NaN en X_exog_train o X_exog_test\")\n",
    "\n",
    "            # Modelo SARIMA\n",
    "            modelo = sm.tsa.SARIMAX(\n",
    "                train, \n",
    "                order=(p, d, q),\n",
    "                seasonal_order=(P, D, Q, s),\n",
    "                enforce_stationarity=True,\n",
    "                enforce_invertibility=True,\n",
    "                exog=X_exog_train\n",
    "            )\n",
    "\n",
    "            # modelo_fit = modelo.fit(method_kwargs={\"maxiter\": 300})\n",
    "            modelo_fit = modelo.fit(disp=False) \n",
    "            predictions = modelo_fit.forecast(steps=len(test), exog=X_exog_test)\n",
    "\n",
    "            # RMSE\n",
    "            split_rmse = np.sqrt(np.mean((predictions - test) ** 2))\n",
    "            errors.append(split_rmse)\n",
    "\n",
    "        avg_rmse = np.mean(errors)\n",
    "        results_sarima.append(((p, d, q, P, D, Q, s), avg_rmse))\n",
    "        print(f\"Finalizado SARIMA({p},{d},{q})({P},{D},{Q},{s}) con RMSE: {avg_rmse}\")\n",
    "        print(\"----------------------------------\")\n",
    "\n",
    "    except ValueError as ve:\n",
    "        print(f\"Error en SARIMA({p},{d},{q})({P},{D},{Q},{s}): ValueError - {ve}\")\n",
    "\n",
    "    except np.linalg.LinAlgError as lae:\n",
    "        print(f\"Error en SARIMA({p},{d},{q})({P},{D},{Q},{s}): LinAlgError - {lae}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error desconocido en SARIMA({p},{d},{q})({P},{D},{Q},{s}): {type(e).__name__} - {e}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Comparar los resultados\n",
    "#best_arima = min(results_arima, key=lambda x: x[1])\n",
    "best_sarima = min(results_sarima, key=lambda x: x[1])\n",
    "\n",
    "#print(f\"Mejor ARIMA: {best_arima[0]} con RMSE {best_arima[1]}\")\n",
    "print(f\"Mejor SARIMA: {best_sarima[0]} con RMSE {best_sarima[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "Con 5 TS-validaciones cruzadas:\n",
    "- Mejor modelo ARIMA(2,1,7) con RMSE: 26.501711547167854\n",
    "- Mejor modelo SARIMAX(2, 1, 6, 4, 1, 2, 7) RMSE 28.537360912596263"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validacion de las predicciones con test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validacion de entrenamiento final con datos separacion de datos para graficar predicciones y datos finales de los ultimos N dias\n",
    "N_forecast = 60\n",
    "y_train = despues_comp_df['demanda'][:-N_forecast]\n",
    "y_test = despues_comp_df['demanda'][-N_forecast:]\n",
    "\n",
    "X_exog = despues_comp_df.loc[:, exog_cols]\n",
    "X_exog_train = X_exog.iloc[:-N_forecast, :]\n",
    "X_exog_test = X_exog.iloc[-N_forecast:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo ARIMA con mejores parametrosy en datos originales\n",
    "modelo_arima = sm.tsa.arima.ARIMA(y_train, order=(2, 1, 7)\n",
    "                            , enforce_stationarity=True\n",
    "                            , enforce_invertibility=True\n",
    "                            , exog=X_exog_train\n",
    "                            )\n",
    "modelo_fit = modelo_arima.fit(method_kwargs={\"maxiter\": 300})\n",
    "\n",
    "# Predicciones en el conjunto de prueba\n",
    "predictions_arima = modelo_fit.forecast(steps=len(y_test), exog=X_exog_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo SARIMA con mejores parametros y en datos originales\n",
    "modelo_sarima = modelo = sm.tsa.SARIMAX(\n",
    "                y_train, \n",
    "                order=(2, 1, 6),\n",
    "                seasonal_order=(4, 1, 2, 7),\n",
    "                enforce_stationarity=True,\n",
    "                enforce_invertibility=True,\n",
    "                exog=X_exog_train\n",
    "            )\n",
    "\n",
    "modelo_fit = modelo_sarima.fit(disp=False) \n",
    "\n",
    "\n",
    "# Predicciones en el conjunto de prueba\n",
    "predictions_sarima = modelo_fit.forecast(steps=len(y_test), exog=X_exog_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculo de RMSE final ARIMA\n",
    "arima_rmse_final = np.sqrt(np.mean((predictions_arima - y_test)**2))\n",
    "print(f\"RMSE ARIMA= {arima_rmse_final}\")\n",
    "\n",
    "# Calculo de RMSE final SARIMA\n",
    "sarima_rmse_final = np.sqrt(np.mean((predictions_sarima - y_test)**2))\n",
    "print(f\"RMSE SARIMA= {sarima_rmse_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,7))\n",
    "plt.plot(despues_comp_df['date'][-N_forecast:], y_test)\n",
    "plt.plot(despues_comp_df['date'][-N_forecast:], predictions_arima, color='r', marker='o')\n",
    "plt.plot(despues_comp_df['date'][-N_forecast:], predictions_sarima, color='g', marker='o')\n",
    "\n",
    "\n",
    "plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m'))\n",
    "\n",
    "plt.title(f\"Test: Demanda real vs Predicciones \")\n",
    "plt.legend(['Demanda real', 'Prediccion ARIMA', 'Prediccion SARIMA'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones con datos originales:\n",
    "- ARIMA: es underfitted, SARIMA es overfitted\n",
    "- Falta hacer las predicciones con solo log() y log() + StandardScaler() **HACER FUNCIONES EN MODULO DE PYTHON**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modulos y secciones para pipelines\n",
    "1. function de evaluacion de si la serie necesita diferenciacion o no para corregir la tendencia (visualmente si tiene tendencia lineal, d=1, si parabolica, d=2)\n",
    "2. la serie es estacional con visualizaciones de la ACF y PACF y la descomposicion de las serie con una estacionalidad predeterminada (s=7)\n",
    "3. dependiendo de la distribucion de los datos, necesita una transformacion logaritmica o mejor una de Box - Cox\n",
    "4. Modulos de TS-CV con parametros a evaluar de modelos ARIMA y SARIMA\n",
    "5. Prediccion de resultados, no necesita (de momento un modulo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Ejecucion de Pipeline con transformacion logaritmica\n",
    "- En teoria no hace falta porque en archivo `predicciones.ipynb` validamos que con la prueba de **Levene** que la varianza no cambia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juanc\\AppData\\Local\\Temp\\ipykernel_25076\\1671687774.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  despues_comp_df['log_demanda'] = np.log(despues_comp_df['demanda'] + 1)\n"
     ]
    }
   ],
   "source": [
    "# Transformacion logaritmica\n",
    "despues_comp_df['log_demanda'] = np.log(despues_comp_df['demanda'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomposicion de la serie con log_demanda, period = 7\n",
    "# Revision visual de la descomposicion de los datos con STL\n",
    "stl = STL(despues_comp_df['log_demanda'], period=7)\n",
    "decomposicion = stl.fit()\n",
    "\n",
    "# Graficar los componentes de la descomposición\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Observación original\n",
    "plt.subplot(411)\n",
    "plt.plot(despues_comp_df['date'], decomposicion.observed)\n",
    "# Formatear los ticks para que muestren solo el mes y el anio\n",
    "plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m'))\n",
    "plt.title('Observación')\n",
    "\n",
    "# Tendencia\n",
    "plt.subplot(412)\n",
    "plt.plot(despues_comp_df['date'], decomposicion.trend)\n",
    "# Formatear los ticks para que muestren solo el mes y el anio\n",
    "plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m'))\n",
    "plt.title('Tendencia')\n",
    "\n",
    "# Estacionalidad\n",
    "plt.subplot(413)\n",
    "plt.plot(despues_comp_df['date'], decomposicion.seasonal)\n",
    "# Formatear los ticks para que muestren solo el mes y el anio\n",
    "plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m'))\n",
    "plt.title('Estacionalidad')\n",
    "\n",
    "# Residual\n",
    "plt.subplot(414)\n",
    "plt.plot(despues_comp_df['date'], decomposicion.resid)\n",
    "# Formatear los ticks para que muestren solo el mes y el anio\n",
    "plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m'))\n",
    "plt.title('Residual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruebas de ACF y PACF\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficas de ACF y PACF\n",
    "# ACF\n",
    "# Funcion de autocorrelacion (ACF): verifica dependencia de ventas pasadas en ventas futuras\n",
    "plot_acf(despues_comp_df['log_demanda'], lags=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial Autocorrelation Function (PACF): identifica que lags son relevantes\n",
    "plot_pacf(despues_comp_df['log_demanda'], lags=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mi conclusion: q = 7 dado que lags 8 a 15 tienen una importancia similar, p = 2\n",
    "- Conclusion viable (IA): (1,1,1) o (2,1,2), considerar q=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validacion Cruzada de modelos ARIMA con (1,1,1) y (2,1,2)\n",
    "# Parámetros a probar\n",
    "p_values = range(1,3)  # AR\n",
    "d_values = [1]  # Ya lo determinaste con ADF\n",
    "q_values = range(5,16)  # MA\n",
    "\n",
    "# Crear combinaciones de p, d, q\n",
    "param_grid = list(itertools.product(p_values, d_values, q_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validacion cruzada con 3 splits\n",
    "tscv = TimeSeriesSplit(n_splits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista que almacena sublistas con la combinacion de (p,d,q) y el promedio de los rmse con esos valores de parametros\n",
    "results = []\n",
    "\n",
    "for p, d ,q in param_grid:\n",
    "    try:\n",
    "        print(f\"Inicio de ejecucion con ({p},{d},{q})\")\n",
    "        errors = []\n",
    "        for train_idx, test_idx in tscv.split(despues_comp_df['log_demanda']):\n",
    "            train, test = despues_comp_df['log_demanda'][train_idx], despues_comp_df['log_demanda'][test_idx]\n",
    "            X_exog = despues_comp_df.loc[:,exog_cols]\n",
    "            X_exog_train = X_exog.iloc[train_idx,:]\n",
    "            X_exog_test = X_exog.iloc[test_idx,:]\n",
    "\n",
    "            # Entrenamiento del modelo ARIMA con parametros actuales\n",
    "            modelo = sm.tsa.arima.ARIMA(train, order=(p, d, q)\n",
    "                                        , enforce_stationarity=False\n",
    "                                        , enforce_invertibility=False\n",
    "                                        , exog=X_exog_train\n",
    "                                        )\n",
    "            modelo_fit = modelo.fit(method_kwargs={\"maxiter\": 100})\n",
    "\n",
    "            # Predicciones en el conjunto de prueba\n",
    "            predictions = modelo_fit.forecast(steps=len(test), exog=X_exog_test)\n",
    "\n",
    "\n",
    "            # Calculo de RMSE para un split\n",
    "            split_rmse = np.sqrt(np.mean((predictions - test)**2))\n",
    "            errors.append(split_rmse)\n",
    "\n",
    "        # Promedio de los errores de os splits\n",
    "        avg_rmse = np.mean(errors)\n",
    "        results.append(((p, d, q), avg_rmse))\n",
    "        print(\"Entrenamiento finalizado\")\n",
    "        print(\"----------------------------------\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error con la combinacion ({p},{d},{q})\")\n",
    "        print(e)\n",
    "        print(\"----------------------------------\")\n",
    "    #    continue # en caso de error se ignora esa combinacion\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar los mejores parámetros (menor RMSE)\n",
    "best_params, best_rmse = min(results, key=lambda x: x[1])\n",
    "\n",
    "print(f\"Mejor configuración de ARIMA: {best_params} con RMSE = {best_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generacion de modelo entrenado final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm  # Para construir y ajustar el modelo ARIMA.\n",
    "from utility_funcs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediccion de resultados con los datos de los ultimos 2 anios\n",
    "train_size = 365*2\n",
    "train = despues_comp_df['log_demanda'][-train_size:]\n",
    "\n",
    "X_exog = despues_comp_df.loc[:,exog_cols]\n",
    "X_exog_train = X_exog.iloc[-train_size:, :]\n",
    "\n",
    "# Entrenamiento del modelo ARIMA con parametros actuales\n",
    "modelo = sm.tsa.arima.ARIMA(train, order=(2, 1, 14)\n",
    "                            , enforce_stationarity=False\n",
    "                            , enforce_invertibility=False\n",
    "                            , exog=X_exog_train\n",
    "                            )\n",
    "modelo_fit = modelo.fit(method_kwargs={\"maxiter\": 200})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['forecast_model_with_exogenous.joblib']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exportacion del modelo con joblib\n",
    "# Model es el mejor modelo\n",
    "model_wrapper = ForecastModelWithExogenous(modelo_fit)\n",
    "joblib.dump(model_wrapper, 'forecast_model_with_exogenous.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-forecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
